{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-9\n",
    "### Python Program for Artificial Neural Network (Manual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128 : \t loss:0.693171\n",
      "Epoch 2/128 : \t loss:0.559884\n",
      "Epoch 3/128 : \t loss:0.523445\n",
      "Epoch 4/128 : \t loss:0.511414\n",
      "Epoch 5/128 : \t loss:0.506760\n",
      "Epoch 6/128 : \t loss:0.504578\n",
      "Epoch 7/128 : \t loss:0.503120\n",
      "Epoch 8/128 : \t loss:0.501564\n",
      "Epoch 9/128 : \t loss:0.499329\n",
      "Epoch 10/128 : \t loss:0.495829\n",
      "Epoch 11/128 : \t loss:0.490441\n",
      "Epoch 12/128 : \t loss:0.482719\n",
      "Epoch 13/128 : \t loss:0.472824\n",
      "Epoch 14/128 : \t loss:0.461878\n",
      "Epoch 15/128 : \t loss:0.451617\n",
      "Epoch 16/128 : \t loss:0.443372\n",
      "Epoch 17/128 : \t loss:0.437473\n",
      "Epoch 18/128 : \t loss:0.433560\n",
      "Epoch 19/128 : \t loss:0.431093\n",
      "Epoch 20/128 : \t loss:0.429586\n",
      "Epoch 21/128 : \t loss:0.428669\n",
      "Epoch 22/128 : \t loss:0.428092\n",
      "Epoch 23/128 : \t loss:0.427699\n",
      "Epoch 24/128 : \t loss:0.427397\n",
      "Epoch 25/128 : \t loss:0.427135\n",
      "Epoch 26/128 : \t loss:0.426880\n",
      "Epoch 27/128 : \t loss:0.426615\n",
      "Epoch 28/128 : \t loss:0.426330\n",
      "Epoch 29/128 : \t loss:0.426018\n",
      "Epoch 30/128 : \t loss:0.425676\n",
      "Epoch 31/128 : \t loss:0.425302\n",
      "Epoch 32/128 : \t loss:0.424895\n",
      "Epoch 33/128 : \t loss:0.424456\n",
      "Epoch 34/128 : \t loss:0.423987\n",
      "Epoch 35/128 : \t loss:0.423488\n",
      "Epoch 36/128 : \t loss:0.422964\n",
      "Epoch 37/128 : \t loss:0.422416\n",
      "Epoch 38/128 : \t loss:0.421847\n",
      "Epoch 39/128 : \t loss:0.421262\n",
      "Epoch 40/128 : \t loss:0.420662\n",
      "Epoch 41/128 : \t loss:0.420053\n",
      "Epoch 42/128 : \t loss:0.419436\n",
      "Epoch 43/128 : \t loss:0.418815\n",
      "Epoch 44/128 : \t loss:0.418193\n",
      "Epoch 45/128 : \t loss:0.417573\n",
      "Epoch 46/128 : \t loss:0.416957\n",
      "Epoch 47/128 : \t loss:0.416347\n",
      "Epoch 48/128 : \t loss:0.415747\n",
      "Epoch 49/128 : \t loss:0.415157\n",
      "Epoch 50/128 : \t loss:0.414579\n",
      "Epoch 51/128 : \t loss:0.414015\n",
      "Epoch 52/128 : \t loss:0.413465\n",
      "Epoch 53/128 : \t loss:0.412931\n",
      "Epoch 54/128 : \t loss:0.412413\n",
      "Epoch 55/128 : \t loss:0.411913\n",
      "Epoch 56/128 : \t loss:0.411429\n",
      "Epoch 57/128 : \t loss:0.410963\n",
      "Epoch 58/128 : \t loss:0.410514\n",
      "Epoch 59/128 : \t loss:0.410083\n",
      "Epoch 60/128 : \t loss:0.409668\n",
      "Epoch 61/128 : \t loss:0.409271\n",
      "Epoch 62/128 : \t loss:0.408890\n",
      "Epoch 63/128 : \t loss:0.408526\n",
      "Epoch 64/128 : \t loss:0.408177\n",
      "Epoch 65/128 : \t loss:0.407843\n",
      "Epoch 66/128 : \t loss:0.407525\n",
      "Epoch 67/128 : \t loss:0.407220\n",
      "Epoch 68/128 : \t loss:0.406930\n",
      "Epoch 69/128 : \t loss:0.406652\n",
      "Epoch 70/128 : \t loss:0.406388\n",
      "Epoch 71/128 : \t loss:0.406135\n",
      "Epoch 72/128 : \t loss:0.405894\n",
      "Epoch 73/128 : \t loss:0.405664\n",
      "Epoch 74/128 : \t loss:0.405445\n",
      "Epoch 75/128 : \t loss:0.405236\n",
      "Epoch 76/128 : \t loss:0.405036\n",
      "Epoch 77/128 : \t loss:0.404846\n",
      "Epoch 78/128 : \t loss:0.404664\n",
      "Epoch 79/128 : \t loss:0.404491\n",
      "Epoch 80/128 : \t loss:0.404325\n",
      "Epoch 81/128 : \t loss:0.404167\n",
      "Epoch 82/128 : \t loss:0.404015\n",
      "Epoch 83/128 : \t loss:0.403871\n",
      "Epoch 84/128 : \t loss:0.403732\n",
      "Epoch 85/128 : \t loss:0.403600\n",
      "Epoch 86/128 : \t loss:0.403473\n",
      "Epoch 87/128 : \t loss:0.403352\n",
      "Epoch 88/128 : \t loss:0.403236\n",
      "Epoch 89/128 : \t loss:0.403125\n",
      "Epoch 90/128 : \t loss:0.403018\n",
      "Epoch 91/128 : \t loss:0.402915\n",
      "Epoch 92/128 : \t loss:0.402816\n",
      "Epoch 93/128 : \t loss:0.402721\n",
      "Epoch 94/128 : \t loss:0.402630\n",
      "Epoch 95/128 : \t loss:0.402542\n",
      "Epoch 96/128 : \t loss:0.402458\n",
      "Epoch 97/128 : \t loss:0.402376\n",
      "Epoch 98/128 : \t loss:0.402297\n",
      "Epoch 99/128 : \t loss:0.402221\n",
      "Epoch 100/128 : \t loss:0.402147\n",
      "Epoch 101/128 : \t loss:0.402076\n",
      "Epoch 102/128 : \t loss:0.402006\n",
      "Epoch 103/128 : \t loss:0.401939\n",
      "Epoch 104/128 : \t loss:0.401874\n",
      "Epoch 105/128 : \t loss:0.401811\n",
      "Epoch 106/128 : \t loss:0.401749\n",
      "Epoch 107/128 : \t loss:0.401689\n",
      "Epoch 108/128 : \t loss:0.401631\n",
      "Epoch 109/128 : \t loss:0.401573\n",
      "Epoch 110/128 : \t loss:0.401518\n",
      "Epoch 111/128 : \t loss:0.401463\n",
      "Epoch 112/128 : \t loss:0.401409\n",
      "Epoch 113/128 : \t loss:0.401357\n",
      "Epoch 114/128 : \t loss:0.401306\n",
      "Epoch 115/128 : \t loss:0.401255\n",
      "Epoch 116/128 : \t loss:0.401205\n",
      "Epoch 117/128 : \t loss:0.401156\n",
      "Epoch 118/128 : \t loss:0.401108\n",
      "Epoch 119/128 : \t loss:0.401060\n",
      "Epoch 120/128 : \t loss:0.401014\n",
      "Epoch 121/128 : \t loss:0.400967\n",
      "Epoch 122/128 : \t loss:0.400921\n",
      "Epoch 123/128 : \t loss:0.400876\n",
      "Epoch 124/128 : \t loss:0.400831\n",
      "Epoch 125/128 : \t loss:0.400786\n",
      "Epoch 126/128 : \t loss:0.400742\n",
      "Epoch 127/128 : \t loss:0.400698\n",
      "Epoch 128/128 : \t loss:0.400655\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow \n",
    "tensorflow.random.set_seed(1)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "# Functions for the ANN Model\n",
    "def initialize_parameters(n_x,n_h,n_y):\n",
    "    W1=np.random.randn(n_h,n_x)*0.01\n",
    "    b1=np.zeros((n_h,1))\n",
    "    W2=np.random.randn(n_y,n_h)*0.01\n",
    "    b2=np.zeros((n_y,1))\n",
    "    parameters={\n",
    "            \"W1\":W1,\n",
    "            \"b1\":b1,\n",
    "            \"W2\":W2,\n",
    "            \"b2\":b2}\n",
    "    return parameters\n",
    "\n",
    "def layer_sizes(X,Y):\n",
    "    n_x=X.shape[0]\n",
    "    n_h=10\n",
    "    n_y=Y.shape[0]\n",
    "    return (n_x,n_h,n_y)\n",
    "\n",
    "def compute_cost(A2,Y,parameters):\n",
    "    m=Y.shape[1]\n",
    "    logprobs=np.multiply(np.log(A2),Y)+np.multiply(np.log(1-A2),(1-Y))\n",
    "    cost=-(1/m)*np.sum(logprobs)\n",
    "    return cost\n",
    "\n",
    "def forward_propagation(X,parameters):\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    \n",
    "    Z1=np.dot(W1,X)+b1\n",
    "    A1=np.tanh(Z1)\n",
    "    Z2=np.dot(W2,A1)+b2\n",
    "    A2=1/(1+np.exp(-Z2))\n",
    "    \n",
    "    cache={\n",
    "            \"Z1\":Z1,\n",
    "            \"A1\":A1,\n",
    "            \"Z2\":Z2,\n",
    "            \"A2\":A2}\n",
    "    return A2,cache\n",
    "\n",
    "def backward_propagation(parameters,cache,X,Y):\n",
    "    m=X.shape[1]\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    \n",
    "    A1=cache[\"A1\"]\n",
    "    A2=cache[\"A2\"]\n",
    "    \n",
    "    dZ2=A2-Y\n",
    "    dW2=(1/m)*np.dot(dZ2,A1.T)\n",
    "    db2=(1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1 = np.multiply((np.dot(W2.T,dZ2)),(1-np.power(A1,2)))\n",
    "    dW1 = 1/(m)*np.dot(dZ1,X.T)\n",
    "    db1 = 1/(m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 0.1):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 = W1-((learning_rate)*dW1)\n",
    "    b1 = b1-((learning_rate)*db1)\n",
    "    W2 = W2-((learning_rate)*dW2)\n",
    "    b2 = b2-((learning_rate)*db2)\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "def predict(parameters, X):\n",
    "    A2, cache = forward_propagation(X,parameters)\n",
    "    predictions = (A2>0.5)*1\n",
    "    return predictions\n",
    "\n",
    "# The ANN Model\n",
    "def nn_model(X, Y, n_h, num_iterations = 128, print_cost=False):\n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "         A2, cache = forward_propagation(X,parameters)\n",
    "         cost = compute_cost(A2,Y,parameters)\n",
    "         grads = backward_propagation(parameters,cache,X,Y)\n",
    "         parameters = update_parameters(parameters,grads,2) \n",
    "         if print_cost:\n",
    "            print (\"Epoch %i/%i : \\t loss:%f\" %(i+1,num_iterations, cost))\n",
    "    return parameters\n",
    "\n",
    "# Input: Dataset \n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# Encoding Categorical Values\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X.fit_transform(X[:, 2])\n",
    "ct = ColumnTransformer([(\"Geography\", OneHotEncoder(), [1])],\n",
    "                       remainder = 'passthrough')\n",
    "X = ct.fit_transform(X)\n",
    "\n",
    "# Splitting the data into training and testing data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Preprocessing Data\n",
    "X_train=X_train.T\n",
    "X_test=X_test.T\n",
    "y_train=y_train.reshape(y_train.shape[0],1)\n",
    "y_test=y_test.reshape(y_test.shape[0],1)\n",
    "y_train=y_train.T\n",
    "y_test=y_test.T\n",
    "shape_X=X_train.shape\n",
    "shape_Y=y_train.shape\n",
    "m=X_train.shape[1]\n",
    "\n",
    "# Train the model using the training sets\n",
    "(n_x,n_h,n_y)=layer_sizes(X_train,y_train)\n",
    "parameters=initialize_parameters(n_x,n_h,n_y)\n",
    "A2,cache=forward_propagation(X_train,parameters)\n",
    "grads = backward_propagation(parameters, cache, X_train, y_train)\n",
    "parameters = update_parameters(parameters, grads)\n",
    "parameters = nn_model(X_train, y_train, 10, num_iterations=128,print_cost=True)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = predict(parameters, X_test)\n",
    "\n",
    "# Calculating the Confusion Matrix and Accuracy of the Model\n",
    "cm = metrics.confusion_matrix(y_test.T, y_pred.T)\n",
    "accuracy = 100*(cm[0][0]+cm[1][1])/X_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-9\n",
    "### Python Program for Artificial Neural Network (Keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "2000/2000 [==============================] - 2s 559us/step - loss: 0.5045 - accuracy: 0.7907\n",
      "Epoch 2/128\n",
      "2000/2000 [==============================] - 1s 586us/step - loss: 0.4325 - accuracy: 0.8119\n",
      "Epoch 3/128\n",
      "2000/2000 [==============================] - 1s 551us/step - loss: 0.4033 - accuracy: 0.8341\n",
      "Epoch 4/128\n",
      "2000/2000 [==============================] - 1s 544us/step - loss: 0.4161 - accuracy: 0.8236\n",
      "Epoch 5/128\n",
      "2000/2000 [==============================] - 1s 532us/step - loss: 0.4012 - accuracy: 0.8389\n",
      "Epoch 6/128\n",
      "2000/2000 [==============================] - 1s 525us/step - loss: 0.4037 - accuracy: 0.8406\n",
      "Epoch 7/128\n",
      "2000/2000 [==============================] - 1s 525us/step - loss: 0.4066 - accuracy: 0.8385\n",
      "Epoch 8/128\n",
      "2000/2000 [==============================] - 1s 520us/step - loss: 0.4120 - accuracy: 0.8358\n",
      "Epoch 9/128\n",
      "2000/2000 [==============================] - 1s 525us/step - loss: 0.3940 - accuracy: 0.8360\n",
      "Epoch 10/128\n",
      "2000/2000 [==============================] - 1s 536us/step - loss: 0.4047 - accuracy: 0.8375\n",
      "Epoch 11/128\n",
      "2000/2000 [==============================] - 1s 566us/step - loss: 0.4072 - accuracy: 0.8326\n",
      "Epoch 12/128\n",
      "2000/2000 [==============================] - 1s 525us/step - loss: 0.3922 - accuracy: 0.8437\n",
      "Epoch 13/128\n",
      "2000/2000 [==============================] - 1s 573us/step - loss: 0.4062 - accuracy: 0.8329\n",
      "Epoch 14/128\n",
      "2000/2000 [==============================] - 1s 583us/step - loss: 0.4053 - accuracy: 0.8319\n",
      "Epoch 15/128\n",
      "2000/2000 [==============================] - 1s 586us/step - loss: 0.3950 - accuracy: 0.8408\n",
      "Epoch 16/128\n",
      "2000/2000 [==============================] - 1s 583us/step - loss: 0.3856 - accuracy: 0.8464\n",
      "Epoch 17/128\n",
      "2000/2000 [==============================] - 1s 596us/step - loss: 0.4056 - accuracy: 0.8344\n",
      "Epoch 18/128\n",
      "2000/2000 [==============================] - 1s 568us/step - loss: 0.4041 - accuracy: 0.8405\n",
      "Epoch 19/128\n",
      "2000/2000 [==============================] - 1s 517us/step - loss: 0.3947 - accuracy: 0.8414\n",
      "Epoch 20/128\n",
      "2000/2000 [==============================] - 1s 552us/step - loss: 0.3853 - accuracy: 0.8476\n",
      "Epoch 21/128\n",
      "2000/2000 [==============================] - 1s 559us/step - loss: 0.3988 - accuracy: 0.8377\n",
      "Epoch 22/128\n",
      "2000/2000 [==============================] - 1s 551us/step - loss: 0.4083 - accuracy: 0.8301\n",
      "Epoch 23/128\n",
      "2000/2000 [==============================] - 1s 551us/step - loss: 0.4056 - accuracy: 0.8331\n",
      "Epoch 24/128\n",
      "2000/2000 [==============================] - 1s 578us/step - loss: 0.3884 - accuracy: 0.8451\n",
      "Epoch 25/128\n",
      "2000/2000 [==============================] - 1s 544us/step - loss: 0.3857 - accuracy: 0.8408\n",
      "Epoch 26/128\n",
      "2000/2000 [==============================] - 1s 559us/step - loss: 0.3974 - accuracy: 0.8400\n",
      "Epoch 27/128\n",
      "2000/2000 [==============================] - 1s 551us/step - loss: 0.3954 - accuracy: 0.8375\n",
      "Epoch 28/128\n",
      "2000/2000 [==============================] - 1s 537us/step - loss: 0.4007 - accuracy: 0.8369\n",
      "Epoch 29/128\n",
      "2000/2000 [==============================] - 1s 563us/step - loss: 0.3881 - accuracy: 0.8435\n",
      "Epoch 30/128\n",
      "2000/2000 [==============================] - 1s 512us/step - loss: 0.3903 - accuracy: 0.8451\n",
      "Epoch 31/128\n",
      "2000/2000 [==============================] - 1s 543us/step - loss: 0.3859 - accuracy: 0.8428\n",
      "Epoch 32/128\n",
      "2000/2000 [==============================] - 1s 557us/step - loss: 0.3951 - accuracy: 0.8413\n",
      "Epoch 33/128\n",
      "2000/2000 [==============================] - 1s 552us/step - loss: 0.3895 - accuracy: 0.8453\n",
      "Epoch 34/128\n",
      "2000/2000 [==============================] - 1s 520us/step - loss: 0.3923 - accuracy: 0.8458\n",
      "Epoch 35/128\n",
      "2000/2000 [==============================] - 1s 552us/step - loss: 0.3824 - accuracy: 0.8458\n",
      "Epoch 36/128\n",
      "2000/2000 [==============================] - 1s 517us/step - loss: 0.3948 - accuracy: 0.8413\n",
      "Epoch 37/128\n",
      "2000/2000 [==============================] - 1s 509us/step - loss: 0.3952 - accuracy: 0.8421\n",
      "Epoch 38/128\n",
      "2000/2000 [==============================] - 1s 520us/step - loss: 0.3884 - accuracy: 0.8479\n",
      "Epoch 39/128\n",
      "2000/2000 [==============================] - 1s 517us/step - loss: 0.3905 - accuracy: 0.8453\n",
      "Epoch 40/128\n",
      "2000/2000 [==============================] - 1s 528us/step - loss: 0.3923 - accuracy: 0.8432\n",
      "Epoch 41/128\n",
      "2000/2000 [==============================] - 1s 552us/step - loss: 0.3893 - accuracy: 0.8452\n",
      "Epoch 42/128\n",
      "2000/2000 [==============================] - 1s 552us/step - loss: 0.3910 - accuracy: 0.8441\n",
      "Epoch 43/128\n",
      "2000/2000 [==============================] - 1s 559us/step - loss: 0.3911 - accuracy: 0.8472\n",
      "Epoch 44/128\n",
      "2000/2000 [==============================] - 1s 559us/step - loss: 0.3929 - accuracy: 0.8463\n",
      "Epoch 45/128\n",
      "2000/2000 [==============================] - 1s 544us/step - loss: 0.3926 - accuracy: 0.8432\n",
      "Epoch 46/128\n",
      "2000/2000 [==============================] - 1s 543us/step - loss: 0.3977 - accuracy: 0.8458\n",
      "Epoch 47/128\n",
      "2000/2000 [==============================] - 1s 559us/step - loss: 0.3867 - accuracy: 0.8465\n",
      "Epoch 48/128\n",
      "2000/2000 [==============================] - 1s 525us/step - loss: 0.3890 - accuracy: 0.8488\n",
      "Epoch 49/128\n",
      "2000/2000 [==============================] - 1s 512us/step - loss: 0.3960 - accuracy: 0.8463\n",
      "Epoch 50/128\n",
      "2000/2000 [==============================] - 1s 540us/step - loss: 0.3901 - accuracy: 0.8400\n",
      "Epoch 51/128\n",
      "2000/2000 [==============================] - 1s 567us/step - loss: 0.3969 - accuracy: 0.8446\n",
      "Epoch 52/128\n",
      "2000/2000 [==============================] - 1s 536us/step - loss: 0.3842 - accuracy: 0.8471\n",
      "Epoch 53/128\n",
      "2000/2000 [==============================] - 1s 548us/step - loss: 0.3904 - accuracy: 0.8485\n",
      "Epoch 54/128\n",
      "2000/2000 [==============================] - 1s 561us/step - loss: 0.3851 - accuracy: 0.8541\n",
      "Epoch 55/128\n",
      "2000/2000 [==============================] - 1s 567us/step - loss: 0.3782 - accuracy: 0.8555\n",
      "Epoch 56/128\n",
      "2000/2000 [==============================] - 1s 520us/step - loss: 0.3884 - accuracy: 0.8489\n",
      "Epoch 57/128\n",
      "2000/2000 [==============================] - 1s 525us/step - loss: 0.3867 - accuracy: 0.8504\n",
      "Epoch 58/128\n",
      "2000/2000 [==============================] - 1s 536us/step - loss: 0.3711 - accuracy: 0.8547\n",
      "Epoch 59/128\n",
      "2000/2000 [==============================] - 1s 544us/step - loss: 0.3528 - accuracy: 0.8650\n",
      "Epoch 60/128\n",
      "2000/2000 [==============================] - 1s 552us/step - loss: 0.3561 - accuracy: 0.8617\n",
      "Epoch 61/128\n",
      "2000/2000 [==============================] - 1s 559us/step - loss: 0.3567 - accuracy: 0.8611\n",
      "Epoch 62/128\n",
      "2000/2000 [==============================] - 1s 567us/step - loss: 0.3532 - accuracy: 0.8636\n",
      "Epoch 63/128\n",
      "2000/2000 [==============================] - 1s 536us/step - loss: 0.3488 - accuracy: 0.8674\n",
      "Epoch 64/128\n",
      "2000/2000 [==============================] - 1s 543us/step - loss: 0.3545 - accuracy: 0.8613\n",
      "Epoch 65/128\n",
      "2000/2000 [==============================] - 1s 564us/step - loss: 0.3466 - accuracy: 0.8640\n",
      "Epoch 66/128\n",
      "2000/2000 [==============================] - 1s 626us/step - loss: 0.3446 - accuracy: 0.8706\n",
      "Epoch 67/128\n",
      "2000/2000 [==============================] - 1s 570us/step - loss: 0.3400 - accuracy: 0.8689\n",
      "Epoch 68/128\n",
      "2000/2000 [==============================] - 1s 551us/step - loss: 0.3469 - accuracy: 0.8666\n",
      "Epoch 69/128\n",
      "2000/2000 [==============================] - 1s 533us/step - loss: 0.3564 - accuracy: 0.8676\n",
      "Epoch 70/128\n",
      "2000/2000 [==============================] - 1s 519us/step - loss: 0.3426 - accuracy: 0.8673\n",
      "Epoch 71/128\n",
      "2000/2000 [==============================] - 1s 507us/step - loss: 0.3435 - accuracy: 0.8649\n",
      "Epoch 72/128\n",
      "2000/2000 [==============================] - 1s 493us/step - loss: 0.3507 - accuracy: 0.8655\n",
      "Epoch 73/128\n",
      "2000/2000 [==============================] - 1s 536us/step - loss: 0.3520 - accuracy: 0.8672\n",
      "Epoch 74/128\n",
      "2000/2000 [==============================] - 1s 551us/step - loss: 0.3508 - accuracy: 0.8645\n",
      "Epoch 75/128\n",
      "2000/2000 [==============================] - 1s 517us/step - loss: 0.3559 - accuracy: 0.8655\n",
      "Epoch 76/128\n",
      "2000/2000 [==============================] - 1s 493us/step - loss: 0.3501 - accuracy: 0.8644\n",
      "Epoch 77/128\n",
      "2000/2000 [==============================] - 1s 528us/step - loss: 0.3533 - accuracy: 0.8656\n",
      "Epoch 78/128\n",
      "2000/2000 [==============================] - 1s 477us/step - loss: 0.3497 - accuracy: 0.8654\n",
      "Epoch 79/128\n",
      "2000/2000 [==============================] - 1s 557us/step - loss: 0.3558 - accuracy: 0.8645\n",
      "Epoch 80/128\n",
      "2000/2000 [==============================] - 1s 536us/step - loss: 0.3546 - accuracy: 0.8636\n",
      "Epoch 81/128\n",
      "2000/2000 [==============================] - 1s 552us/step - loss: 0.3609 - accuracy: 0.8606\n",
      "Epoch 82/128\n",
      "2000/2000 [==============================] - 1s 525us/step - loss: 0.3532 - accuracy: 0.8687\n",
      "Epoch 83/128\n",
      "2000/2000 [==============================] - 1s 492us/step - loss: 0.3544 - accuracy: 0.8653\n",
      "Epoch 84/128\n",
      "2000/2000 [==============================] - 1s 525us/step - loss: 0.3450 - accuracy: 0.8693\n",
      "Epoch 85/128\n",
      "2000/2000 [==============================] - 1s 564us/step - loss: 0.3456 - accuracy: 0.8689\n",
      "Epoch 86/128\n",
      "2000/2000 [==============================] - 1s 525us/step - loss: 0.3470 - accuracy: 0.8674\n",
      "Epoch 87/128\n",
      "2000/2000 [==============================] - 1s 492us/step - loss: 0.3426 - accuracy: 0.8732\n",
      "Epoch 88/128\n",
      "2000/2000 [==============================] - 1s 515us/step - loss: 0.3526 - accuracy: 0.8615\n",
      "Epoch 89/128\n",
      "2000/2000 [==============================] - 1s 556us/step - loss: 0.3567 - accuracy: 0.8644\n",
      "Epoch 90/128\n",
      "2000/2000 [==============================] - 1s 504us/step - loss: 0.3650 - accuracy: 0.8625\n",
      "Epoch 91/128\n",
      "2000/2000 [==============================] - 1s 524us/step - loss: 0.3654 - accuracy: 0.8603\n",
      "Epoch 92/128\n",
      "2000/2000 [==============================] - 1s 513us/step - loss: 0.3503 - accuracy: 0.8703\n",
      "Epoch 93/128\n",
      "2000/2000 [==============================] - 1s 527us/step - loss: 0.3419 - accuracy: 0.8729\n",
      "Epoch 94/128\n",
      "2000/2000 [==============================] - 1s 525us/step - loss: 0.3561 - accuracy: 0.8617\n",
      "Epoch 95/128\n",
      "2000/2000 [==============================] - 1s 546us/step - loss: 0.3577 - accuracy: 0.8673\n",
      "Epoch 96/128\n",
      "2000/2000 [==============================] - 1s 505us/step - loss: 0.3606 - accuracy: 0.8618\n",
      "Epoch 97/128\n",
      "2000/2000 [==============================] - 1s 548us/step - loss: 0.3610 - accuracy: 0.8645\n",
      "Epoch 98/128\n",
      "2000/2000 [==============================] - 1s 573us/step - loss: 0.3442 - accuracy: 0.8707\n",
      "Epoch 99/128\n",
      "2000/2000 [==============================] - 1s 567us/step - loss: 0.3444 - accuracy: 0.8704\n",
      "Epoch 100/128\n",
      "2000/2000 [==============================] - 1s 575us/step - loss: 0.3653 - accuracy: 0.8627\n",
      "Epoch 101/128\n",
      "2000/2000 [==============================] - 1s 594us/step - loss: 0.3496 - accuracy: 0.8690\n",
      "Epoch 102/128\n",
      "2000/2000 [==============================] - 1s 586us/step - loss: 0.3440 - accuracy: 0.8721\n",
      "Epoch 103/128\n",
      "2000/2000 [==============================] - 1s 571us/step - loss: 0.3683 - accuracy: 0.8664\n",
      "Epoch 104/128\n",
      "2000/2000 [==============================] - 1s 559us/step - loss: 0.3586 - accuracy: 0.8656\n",
      "Epoch 105/128\n",
      "2000/2000 [==============================] - 1s 567us/step - loss: 0.3728 - accuracy: 0.8568\n",
      "Epoch 106/128\n",
      "2000/2000 [==============================] - 1s 567us/step - loss: 0.3406 - accuracy: 0.8766\n",
      "Epoch 107/128\n",
      "2000/2000 [==============================] - 1s 528us/step - loss: 0.3529 - accuracy: 0.8699\n",
      "Epoch 108/128\n",
      "2000/2000 [==============================] - 1s 532us/step - loss: 0.3568 - accuracy: 0.8644\n",
      "Epoch 109/128\n",
      "2000/2000 [==============================] - 1s 605us/step - loss: 0.3594 - accuracy: 0.8626\n",
      "Epoch 110/128\n",
      "2000/2000 [==============================] - 1s 602us/step - loss: 0.3564 - accuracy: 0.8657\n",
      "Epoch 111/128\n",
      "2000/2000 [==============================] - 1s 575us/step - loss: 0.3496 - accuracy: 0.8673\n",
      "Epoch 112/128\n",
      "2000/2000 [==============================] - 1s 594us/step - loss: 0.3615 - accuracy: 0.8660\n",
      "Epoch 113/128\n",
      "2000/2000 [==============================] - 1s 586us/step - loss: 0.3418 - accuracy: 0.8725\n",
      "Epoch 114/128\n",
      "2000/2000 [==============================] - 1s 628us/step - loss: 0.3589 - accuracy: 0.8638\n",
      "Epoch 115/128\n",
      "2000/2000 [==============================] - 1s 598us/step - loss: 0.3579 - accuracy: 0.8658\n",
      "Epoch 116/128\n",
      "2000/2000 [==============================] - 1s 620us/step - loss: 0.3513 - accuracy: 0.8622\n",
      "Epoch 117/128\n",
      "2000/2000 [==============================] - 1s 623us/step - loss: 0.3558 - accuracy: 0.8695\n",
      "Epoch 118/128\n",
      "2000/2000 [==============================] - 1s 603us/step - loss: 0.3514 - accuracy: 0.8691\n",
      "Epoch 119/128\n",
      "2000/2000 [==============================] - 1s 609us/step - loss: 0.3613 - accuracy: 0.8657\n",
      "Epoch 120/128\n",
      "2000/2000 [==============================] - 1s 586us/step - loss: 0.3489 - accuracy: 0.8701\n",
      "Epoch 121/128\n",
      "2000/2000 [==============================] - 1s 536us/step - loss: 0.3495 - accuracy: 0.8659\n",
      "Epoch 122/128\n",
      "2000/2000 [==============================] - 1s 578us/step - loss: 0.3565 - accuracy: 0.8697\n",
      "Epoch 123/128\n",
      "2000/2000 [==============================] - 1s 610us/step - loss: 0.3495 - accuracy: 0.8722\n",
      "Epoch 124/128\n",
      "2000/2000 [==============================] - 1s 602us/step - loss: 0.3470 - accuracy: 0.8668\n",
      "Epoch 125/128\n",
      "2000/2000 [==============================] - 1s 624us/step - loss: 0.3608 - accuracy: 0.8631\n",
      "Epoch 126/128\n",
      "2000/2000 [==============================] - 1s 586us/step - loss: 0.3505 - accuracy: 0.8726\n",
      "Epoch 127/128\n",
      "2000/2000 [==============================] - 1s 583us/step - loss: 0.3603 - accuracy: 0.8645\n",
      "Epoch 128/128\n",
      "2000/2000 [==============================] - 1s 578us/step - loss: 0.3451 - accuracy: 0.8677\n"
     ]
    }
   ],
   "source": [
    "# Input: Dataset \n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# Encoding Categorical Values\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X.fit_transform(X[:, 2])\n",
    "ct = ColumnTransformer([(\"Geography\", OneHotEncoder(), [1])],\n",
    "                       remainder = 'passthrough')\n",
    "X = ct.fit_transform(X)\n",
    "\n",
    "# Splitting the data into training and testing data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# The ANN Model\n",
    "def deep_model():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units=12, kernel_initializer='uniform',\n",
    "                         activation='relu', input_dim=12))\n",
    "    classifier.add(Dense(units=12, kernel_initializer='uniform',\n",
    "                         activation='relu'))\n",
    "    classifier.add(Dense(units=1, kernel_initializer='uniform', \n",
    "                         activation='sigmoid'))\n",
    "    classifier.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "# Create ANN object\n",
    "classifier = deep_model()\n",
    "\n",
    "# Train the model using the training sets\n",
    "classifier.fit(X_train, y_train, batch_size=4, epochs=128)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred1 = classifier.predict(X_test)\n",
    "y_pred1 = (y_pred1 > 0.5)\n",
    "\n",
    "# Calculating the Confusion Matrix and Accuracy of the Model\n",
    "cm1 = metrics.confusion_matrix(y_test, y_pred1)\n",
    "accuracy1 = (cm1[0][0]+cm1[1][1])/(cm1[0][0]+cm1[0][1]+cm1[1][0]+cm1[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-9 \n",
    "### Output and Comparison of Both Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ACCURACY METRIC OF ARTIFICIAL NEURAL NETWORK (MANUAL)\n",
      "\n",
      "The Confusion Matrix for the ANN Model\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEWCAYAAACZnQc8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjNklEQVR4nO3deZwU1bn/8c8XiAIqArIv7qhR4xKVuETFuGuiGPVeNFGuIaLGqPHGuCXRqxGvWY27oqgYF4IxKvEXEUNcsrgRNSooSkRhZBcIiF5hZp7fH1WDzdAz3TNMT/cU37evek3VqVNVp1pez5x56vQpRQRmZpYN7crdADMzazkO6mZmGeKgbmaWIQ7qZmYZ4qBuZpYhDupmZhnioG5mliEO6hkj6WlJSyRtWO62VBpJ50iaKWmZpCmSvpyz72eSZqf73pf0w0bOc6mkj3KWTyTVSuqR7r9b0sp6ddqn+zaV9ISkpZLuqytP990u6bhSfgaWfQ7qGSJpS2B/IIBjWvnaHVrzek0l6UvANcAJwKbAGODhnKA6BtghIroA+wInS/p6vnNFxNURsXHdAvwUeDoiFuVU+1lunYioScvPAF4BegNbAsel7dsH6BsRD7fgbdt6yEE9W04FngfuBobn7pA0UNLvJS2U9KGkG3P2nS7pTUnLJU2T9MW0PCRtm1PvbklXpetDJFVJukjSPOAuSd0kPZZeY0m6PiDn+O6S7pI0J93/SFr+hqSv5dT7nKRFknZrwc9mS2BqRPwjkq9R3wP0AHoBRMT0iFiRU78W2Hats9QjScApwNgi27EV8FREfAr8Bdg6/cVyLXBekecwa5CDeracCtyXLodL6g2QBo3HgPdJglt/YFy670Tgf9Jju5D08D8s8np9gO7AFsBIkn9Pd6XbmwOfADfm1P8N0BnYiSSYXpuW3wN8M6feUcDciHi1/gUlbZ6mLhpaTm6grY8D7SV9Kf08vgW8CszLOffFkj4CqoCNgPuL+Az2J+l1P1Sv/DuSFkv6h6Tjc8rfAA6R1Ck9dipwLvB4RPyriOuZNS4ivGRgAb4MrAJ6pNtvAeen6/sAC4EOeY57AjivgXMGsG3O9t3AVen6EGAl0LGRNu0GLEnX+5L0frvlqdcPWA50Sbd/B1zYwp+PgEvTz6gaWATs1UC93YErgE2KOO8Y4O56ZV8ENgM6kPyCWg7sl+7rCIwGXiNJBw0AXiZJCd0CPFv3GXvx0pzFPfXsGA5Mis/yuvfzWQpmIPB+RFTnOW4g0Nwe4sKI+L+6DUmdJd2WPmhcRhKguqY944HA4ohYUv8kETEH+BtwvKSuwJEkf220pG+T9M53AjYg+cvgMUn96rUlIuIVkr8yrmjshGlv+0TqpV4i4uWI+DAiqiPijyT38vV03/9FxMiI2CUiLib5a+VS4BtAe+BA4EuSjljnO7b1koN6BqTB5T+AAyXNS3Pc5wO7StoVmA1s3sDDzNnANg2c+mOSdEmdPvX215/i8/vA9sCXInngeEBdE9PrdE+Ddj5jSQLticBzEfFBvkpp+uWjRpZvNHD+XYE/RMTbEVEbEROBuSQPRfPpQMOfS52vA4uBpwvUC5LPoP69HAEobcsXgCkREcAUYJcC5zTLy0E9G4YCNcCOJCmP3YDPkzyIOxV4kSSAXSNpI0kdJe2XHnsHcIGkPZTYVtIW6b5XSUaBtE8D0IEF2rEJSQ93qaTuwOV1OyJiLkle++b0gernJB2Qc+wjJGmL80hy7HlFxKxYc1RJ/aWhHv5LwNGStk7v81BgO+ANSe0knZG2S5IGA2cDkwvc73DgnjQQrybpBEkbp+c9jOSX1YR6dTqSpF/OT4tmAkMkbQDsB7xb4Npm+ZU7/+Nl3RdgIvDLPOX/QfIgsAPJg8tHSB6CLgKuz6l3JjAd+IjkQd7uafmeJA/ylpM85HyANXPqVfWu14+k1/oR8DbJ8L0gzeWTPFQdC8wHlgC/r3f8HcAKYOMSfEYCrgRmpffzJnBKuq9d+hkuzmn7pSS96LrjPwL2z9nuT5Kb3zbPtf4C/BtYBvwTGJanzpXAD3K2NwUmpcfdD7Qv978rL21zUYRfkmGVQdJlwHYR8c2Clc0sr4r+woitP9J0zQiSMd9m1kzOqVvZSTqd5EHq4xHxbLnbY9aWOf1iZpYh7qmbmWVIxebUVy16139C2Fo69du/3E2wClS98oO1vgfQVE2JOZ/rsfU6X69U3FM3M8uQiu2pm5m1qtqawnXaAAd1MzOAmnxTI7U9DupmZkBEbbmb0CIc1M3MAGod1M3MssM9dTOzDPGDUjOzDHFP3cwsO8KjX8zMMsQPSs3MMsTpFzOzDPGDUjOzDHFP3cwsQ/yg1MwsQ/yg1MwsOyKykVP3fOpmZpDk1ItdCpB0p6QFkt7Is+8CSSGpR07ZJZJmSJou6fCc8j0kvZ7uu15SwZdzOKibmUGSfil2Kexu4Ij6hZIGAocCs3LKdgSGATulx9wsqX26+xZgJDAoXdY6Z30O6mZm0KI99Yh4FlicZ9e1wIVA7qvzjgXGRcSnETETmAEMltQX6BIRz0VEAPcAQwtd2zl1MzOAmlVFV5U0kqQHXWd0RIwucMwxwAcR8c96WZT+wPM521Vp2ap0vX55oxzUzcygSaNf0gDeaBDPJakz8EPgsHy7812ikfJGOaibmUGpv3y0DbAVUNdLHwC8LGkwSQ98YE7dAcCctHxAnvJGOaduZgYt/aB0DRHxekT0iogtI2JLkoD9xYiYB0wAhknaUNJWJA9EX4yIucBySXuno15OBR4tdC0HdTMzaNGgLukB4Dlge0lVkkY0VDcipgLjgWnARODs+GzQ/FnAHSQPT/8FPF7o2k6/mJkB0YQHpQXPFXFSgf1b1tseBYzKU28KsHNTru2gbmYGntDLzCxTPPeLmVmGuKduZpYh7qmbmWWIe+pmZhlS7ZdkmJllh3vqZmYZ4py6mVmGuKduZpYh7qmbmWWIe+pmZhni0S9mZhkSBd8/0SY4qJuZgXPqZmaZ4qBuZpYhflBqZpYhNTWF67QBDupmZuD0i5lZpjiom5llSEZy6u3K3QAzs0oQtVH0UoikOyUtkPRGTtnPJb0l6TVJD0vqmrPvEkkzJE2XdHhO+R6SXk/3XS9Jha7toG5mBkn6pdilsLuBI+qVPQnsHBG7AG8DlwBI2hEYBuyUHnOzpPbpMbcAI4FB6VL/nGtxUDczg2T0S7FLARHxLLC4XtmkiKibi+B5YEC6fiwwLiI+jYiZwAxgsKS+QJeIeC4iArgHGFro2g7qZmbQpJ66pJGSpuQsI5t4tW8Bj6fr/YHZOfuq0rL+6Xr98kb5QamZGTRp9EtEjAZGN+cykn4IVAP31RXlu0Qj5Y1yUC+TH139K57924t079aVR+69FYCbxtzLQxMm0q3rpgCcd8ZwDth3MK9Pm87//PR6AILgO9/6BoccuB8A//XdC1m0aDEbbrghAKN/PYrNunVt/Ruykpvx9vMs/+gjampqqa6uZu99jqJbt648cN8tbLHFQN5/fzbDTj6TpUv/Xe6mtk2tMKGXpOHAV4GD05QKJD3wgTnVBgBz0vIBecob5aBeJkOPOpSTjz+GS3/yizXKT/nPoZx28glrlG279Rb8dsz1dOjQnoWLFnP88O8wZL+96dAheZZyzeUXsvPnt2u1tlv5HHLoiXz44ZLV2xddeDZ/fuqv/OznN3HhD87mogvP5pJLry5jC9uwEo9Tl3QEcBFwYER8nLNrAnC/pF8B/UgeiL4YETWSlkvaG3gBOBW4odB1SpZTl7SDpIvSYTjXpeufL9X12po9d/sCm3bZpKi6nTp2XB3AP125EgqParL1xNe+djj3/OZBAO75zYMcc0zBwRHWkNoofilA0gPAc8D2kqokjQBuBDYBnpT0qqRbASJiKjAemAZMBM6OiLqnsWcBd5A8PP0Xn+XhG1SSnrqki4CTgHHAi2nxAOABSeMi4ppSXDcLHnjoD0yYOJmddhjED757+urA/9rUt/jx1dcyZ/4C/vfHF6wO8gA/vvpa2rVrx6FD9uOM/zqJIoayWhsUETz+xweICG6//V7uGHMfvXv1YN68BQDMm7eAXj03K3Mr27AWnPslIk7KUzymkfqjgFF5yqcAOzfl2qVKv4wAdoqIVbmF6Z8XU4G8QT19gjwS4OZfXsW3T833uWTXfx53NGemQfmG2+/h5zfezlWX/jcAu+y0A4/edxv/em8WP7zql+y/915suOEG/PTyC+ndswcrVnzM9354FRMmTubYIw8p851YKRwwZChz586nZ8/NmPj4OKZPn1HuJmVKZGSagFKlX2pJckP19U335RURoyNiz4jYc30L6AA9unejffv2tGvXjhOOOZI3pr29Vp1tttycTh078s677wHQu2cPADbaqDNHH3pQ3mMsG+bOnQ/AwoUf8uijj7PXXrsxf8Ei+vTpBUCfPr1YsPDDcjaxbWvB9Es5lSqofw+YLOlxSaPTZSIwGTivRNds8xYu+uy7CpOf+Tvbbr0FAFVz5lFdnfxpOGfefN6bVUX/vr2prq5hSTrSYVV1Nc/8/YXVx1i2dO7ciY033mj1+qGHHMjUqdN57A+TOPWUEwE49ZQT+cMfnihnM9u2qC1+qWAlSb9ExERJ2wGDSQbLi2R4zks5DwDWaz+4/BpeeuU1li5dxsFDv8l3RpzCS6+8xvR33gVB/z69ufzCcwF4+bWpjPnNeDp06EC7duJHF5xNt66b8vEn/8cZ//0jVlVXU1tTy9577c4JflCWSb179+R3DyYp2Q4d2jNu3CM8MelpXpryT8bdfyun/ddJzJ79Af950hllbmkbVuE98GIpKvRlq6sWvVuZDbOy6tRv/3I3wSpQ9coP1nl0wIrLhhUdcza6clzFjkbwOHUzM6j4tEqxHNTNzCAz6RcHdTMzsjOk0UHdzAzcUzczyxQHdTOzDGnBaQLKyUHdzAyKevdoW+CgbmYGTr+YmWWKR7+YmWWIe+pmZhnioG5mlh1R4/SLmVl2uKduZpYdHtJoZpYlDupmZhmSjZR6yV5nZ2bWpkR1bdFLIZLulLRA0hs5Zd0lPSnpnfRnt5x9l0iaIWm6pMNzyveQ9Hq673pJBV/O4aBuZgZJT73YpbC7gfrvlrwYmBwRg0je13wxgKQdgWHATukxN0tqnx5zCzASGJQuBd9X6aBuZkbyoLTYpeC5Ip4FFtcrPhYYm66PBYbmlI+LiE8jYiYwAxgsqS/QJSKei+S9o/fkHNMgB3UzM2hST13SSElTcpaRRVyhd0TMBUh/9krL+wOzc+pVpWX90/X65Y3yg1IzM5o2pDEiRgOjW+jS+fLk0Uh5o9xTNzODls6p5zM/TamQ/lyQllcBA3PqDQDmpOUD8pQ3qsGgLmm5pGXpsjxne7mkZU28GTOzihbVxS/NNAEYnq4PBx7NKR8maUNJW5E8EH0xTdEsl7R3Ourl1JxjGtRg+iUiNml2083M2phowXHqkh4AhgA9JFUBlwPXAOMljQBmAScCRMRUSeOBaUA1cHZE1L2G6SySkTSdgMfTpfFrJw9VCzbwy8CgiLhLUg9gk/QpbcmsWvRuNr7eZS2qU7/9y90Eq0DVKz8oOH67kEWHH1h0zOnxxDPrfL1SKfigVNLlwJ7A9sBdwAbAvcB+pW2amVnracmeejkVM/rlOGB34GWAiJgjyakZM8uU9Smor4yIkBQAkjYqcZvMzFpd1FRsRqVJihnSOF7SbUBXSacDfwJuL22zzMxaV9QWv1Sygj31iPiFpEOBZcB2wGUR8WTJW2Zm1oqiNhs99WK/Ufo6yZCaSNfNzDKl0nvgxSqYfpH0beBF4OvACcDzkr5V6oaZmbWmCBW9VLJieuo/AHaPiA8BJG0G/B24s5QNMzNrTVnpqRcT1KuA5Tnby1lzRjEzszavNiOjXxoM6pL+O139AHhB0qMkOfVjSdIxZmaZsT48KK37gtG/0qVOwQllzMzamswH9Yi4ojUbYmZWTkVMg9UmFDP3S0/gQpL353WsK4+Ir5SwXWZmrSorPfVivlF6H/AWsBVwBfAe8FIJ22Rm1uqyMqSxmKC+WUSMAVZFxDMR8S1g7xK3y8ysVdXUqOilkhUzpHFV+nOupKNJXqc0oJH6ZmZtTqX3wItVTFC/StKmwPeBG4AuwPklbZWZWSvLSk69mAm9HktX/w0cVNrmmJmVR+ZHv0i6geTLRnlFxLklaZGZWRmsDz31Ka3WCjOzMqupLWbcSOVr7MtHY1uzIWZm5dSS6RdJ5wPf5rPpyk8DOgO/BbYkGRr+HxGxJK1/CTACqAHOjYgnmnvtbPxqMjNbR7WhopfGSOoPnAvsGRE7A+2BYcDFwOSIGARMTreRtGO6fyfgCOBmSe2bex8O6mZmtPiXjzoAnSR1IOmhzyGZDLEuAzIWGJquHwuMi4hPI2ImMAMY3Nz7cFA3MyNJvxS7SBopaUrOMvKz88QHwC+AWcBc4N8RMQnoHRFz0zpzgV7pIf1ZczrzqrSsWSp29MsBu44o5emtjerRuUu5m2AZVSitkisiRgOj8+2T1I2k970VsBR4UNI3Gzldvgs3O8Pv0S9mZrTo6JdDgJkRsRBA0u+BfYH5kvpGxFxJfYEFaf0qYGDO8QNI0jXN4tEvZmasQ9d4bbOAvSV1Bj4BDibpJK8AhgPXpD/r3k0xAbhf0q+AfsAg1uFFRMVOvXsRsCOeetfMMqop6ZfGRMQLkn4HvAxUA6+QpGo2BsZLGkES+E9M60+VNB6YltY/OyJqmnv9YuZ+uY9kbOXRwJkkv2EWNveCZmaVqCUn9IqIy4HL6xV/StJrz1d/FDCqJa7tqXfNzIDaJiyVzFPvmpkBkXcQStvjqXfNzIDq9WU+dU+9a2brg/Wmpy7pLvKM9klz62ZmmVDpufJiFZN+eSxnvSNwHOswMN7MrBKtNz31iHgod1vSA8CfStYiM7MyWJ966vUNAjZv6YaYmZVTzfrSU5e0nDVz6vNIvmFqZpYZGXmbXVHpl01aoyFmZuVUm5GeesFvlEqaXEyZmVlbFk1YKllj86l3JHljR490fuC6X2NdSGYSMzPLjPXhQekZwPdIAvg/+CyoLwNuKm2zzMxaV62ykX5pbD7164DrJJ0TETe0YpvMzFpds+e6rTDFzNJYK6lr3YakbpK+U7ommZm1vloVv1SyYoL66RGxtG4jIpYAp5esRWZmZVCLil4qWTFfPmonSRERAJLaAxuUtllmZq2r0ke1FKuYoP4EySuYbiW57zOBiSVtlZlZK6v0tEqxignqFwEjgbNIRsBMAm4vZaPMzFpbVoY0FsypR0RtRNwaESdExPHAVJKXZZiZZUaNil8qWTEPSpG0m6SfSnoP+AnwVklbZWbWylryHaWSukr6naS3JL0paR9J3SU9Kemd9Ge3nPqXSJohabqkw9flPhoM6pK2k3SZpDeBG4EqQBFxkMetm1nWtPCLp68DJkbEDsCuwJvAxcDkiBgETE63kbQjMAzYCTgCuDkdkNIsjfXU3wIOBr4WEV9OA3lWxuebma0hVPzSGEldgAOAMQARsTIdFn4sMDatNhYYmq4fC4yLiE8jYiYwAxjc3PtoLKgfTzLN7lOSbpd0MFT4AE0zs2ZqSk9d0khJU3KWkTmn2hpYCNwl6RVJd0jaCOgdEXMB0p+90vr9gdk5x1elZc3S2DQBDwMPp40ZCpwP9JZ0C/BwRExq7kXNzCpNU9IQETEaGN3A7g7AF4FzIuIFSdeRploakK+z3Oxh88WMflkREfdFxFeBAcCrNN5AM7M2pwWnCagCqiLihXT7dyRBfr6kvgDpzwU59QfmHD+AdXgPdFGjX+pExOKIuC0ivtLcC5qZVaKWelAaEfOA2ZK2T4sOBqYBE4Dhadlw4NF0fQIwTNKGkrYieWXoi829j+a8o9TMLHNa+MtH5wD3SdoAeBc4jaQTPV7SCGAWcCJAREyVNJ4k8FcDZ0dEswelOKibmdGyc79ExKvAnnl2HdxA/VHAqJa4toO6mRnr19wvZmaZl5Uv4Tiom5kBtRmZfNdB3cyM7MzS6KBuZsb69ZIMM7PMc0/dzCxDqpWNvrqDupkZTr+YmWWK0y9mZhniIY1mZhmSjZDuoG5mBjj9YmaWKTUZ6as7qJuZ4Z66mVmmhHvqZmbZ4Z66tYhe/Xpy2XWXsFnP7tTWBo/e9xjjxzzET265jM23SV5buEmXjVm+7COGH3Y6fQb0ZtzTY3n/3eTl41NfnsbPLr62nLdgJXLtjVdx6OFDWLRwMUP2PWaNfWd99zQuv+pCdtx6HxYvXgrAOeefzsmnHE9NTS0/umgUT//5b2VoddvlIY3WImqqa7j+ilt4+4136LxRJ+6aeBsvPjuFH5915eo651x2FiuWrVi9XfX+HIYfdno5mmut6Lf3P8Kdt9/PDbdcs0Z5v/59OOCgfama/dm7ibfbfhuGHn8UB+79Nfr07cX4R+5k3z2OpLY2K/3P0stGSG/ii6et5X24YDFvv/EOAB+v+IT33plFzz491qhz8NeGMOnRyeVonpXR83+fwtIlS9cqv/Lqi/nJ5b8g4rMwdPhRX+GRh/7IypWrmPX+B8x8dxa777FLK7a27asmil4qmYN6BekzoDfb7bwtU195c3XZbl/ahcULl1A184PVZf0278PYJ0Zz8+9+za6Dv1COplqZHHbkQcydO59pb0xfo7xv397M+WDe6u25c+bTt2+v1m5emxZN+K+StXpQl3RaI/tGSpoiacr8FXMaqpZJnTp35H9vv5JfX34TH3/08eryQ4d+hSdzeukfLljM0MHDGH74SK674mauuOlHdN64czmabK2sU6eOfO/7Z/Czq29Ya5+09gs2Kz34VJraJizFkNRe0iuSHku3u0t6UtI76c9uOXUvkTRD0nRJh6/LfZSjp35FQzsiYnRE7BkRe/beqF9rtqms2ndoz9W3X8kTD/+JZx7/y2fl7dsx5Mj9+dOEp1aXrVq5imVLlgEw/fW3+eC9OWy+9YBWb7O1vi22GsjmWwzgz399hJde+xN9+/Vm0jMP0bNXD+bMmUe//n1W1+3brzfz5i4sY2vbnhL01M8D3szZvhiYHBGDgMnpNpJ2BIYBOwFHADdLat/c+yhJUJf0WgPL60DvUlyzLfvhLy/k/RnvM270g2uU77X/Hrw/YzYL5y5aXda1+6a0a5f8b+u3eV8GbtWfObPmtmp7rTzemvYOOw/6Mnvtcgh77XIIc+fM57ADj2fhgkVMevwphh5/FBts8Dk236I/W2+zBa/847VyN7lNacmeuqQBwNHAHTnFxwJj0/WxwNCc8nER8WlEzARmAIObex+lGv3SGzgcWFKvXMDfS3TNNmmXvXbmyBMOY8a0fzF20u0A3HrNHTz35xc45Ng1Uy8Au+29K6dfcBo1NTXU1tTws0uuZdnS5eVoupXYLXf8gn2/PJjum3Xl5alP8fNrbuSB3zyUt+70t2Yw4eGJPPvCY1RX13DJBT/xyJcmqokWTVf9GrgQ2CSnrHdEzAWIiLmS6h569Aeez6lXlZY1i6JlbyQ5qTQGuCsi/ppn3/0RcXKhc+zT/yAnBG0tM1fMK1zJ1jvzlr659kOFJjp5i+OKjjkPzHrkDGBkTtHoiBgNIOmrwFER8R1JQ4ALIuKrkpZGRNe6AyQtiYhukm4CnouIe9PyMcAfIyL/b/ACStJTj4gRjewrGNDNzFpbUx4spwF8dAO79wOOkXQU0BHoIuleYL6kvmkvvS+wIK1fBQzMOX4A0OyRIh7SaGZGy+XUI+KSiBgQEVuSPAD9c0R8E5gADE+rDQceTdcnAMMkbShpK2AQ8GJz78PfKDUzo1WmCbgGGC9pBDALOBEgIqZKGg9MA6qBsyOiprkXcVA3M6M04/oj4mng6XT9Q+DgBuqNAka1xDUd1M3MaPHRL2XjoG5mhmdpNDPLlKyM6ndQNzMjO3PlOKibmeH0i5lZppTi2/Xl4KBuZgbUuKduZpYdTr+YmWWI0y9mZhninrqZWYZ4SKOZWYZ4mgAzswxx+sXMLEMc1M3MMsSjX8zMMsQ9dTOzDPHoFzOzDKmJbEy+66BuZoZz6mZmmeKcuplZhmQlp96u3A0wM6sEtRFFL42RNFDSU5LelDRV0nlpeXdJT0p6J/3ZLeeYSyTNkDRd0uHrch8O6mZmJD31Yv8roBr4fkR8HtgbOFvSjsDFwOSIGARMTrdJ9w0DdgKOAG6W1L659+GgbmZGMvql2KUxETE3Il5O15cDbwL9gWOBsWm1scDQdP1YYFxEfBoRM4EZwODm3oeDupkZTUu/SBopaUrOMjLfOSVtCewOvAD0joi5kAR+oFdarT8wO+ewqrSsWfyg1MyMpj0ojYjRwOjG6kjaGHgI+F5ELJPUYNW8zWkmB3UzMyj4ALQpJH2OJKDfFxG/T4vnS+obEXMl9QUWpOVVwMCcwwcAc5p7badfzMxouQelSrrkY4A3I+JXObsmAMPT9eHAoznlwyRtKGkrYBDwYnPvwz11MzOgJmpa6lT7AacAr0t6NS27FLgGGC9pBDALOBEgIqZKGg9MIxk5c3ZE8xvjoG5mRstNExARfyV/nhzg4AaOGQWMaonrO6ibmeFpAszMMsUTepmZZUhLjn4pJwd1MzOyM6GXg7qZGX5JhplZpjinbmaWIc6pm5lliHvqZmYZ4nHqZmYZ4p66mVmGePSLmVmG+EGpmVmGOP1iZpYh/kapmVmGuKduZpYhWcmpKyu/nbJM0sj0Rbdmq/nfheXjd5S2DSPL3QCrSP53YWtxUDczyxAHdTOzDHFQbxucN7V8/O/C1uIHpWZmGeKeuplZhjiom5lliIN6hZN0hKTpkmZIurjc7bHyk3SnpAWS3ih3W6zyOKhXMEntgZuAI4EdgZMk7VjeVlkFuBs4otyNsMrkoF7ZBgMzIuLdiFgJjAOOLXObrMwi4llgcbnbYZXJQb2y9Qdm52xXpWVmZnk5qFc25SnzGFQza5CDemWrAgbmbA8A5pSpLWbWBjioV7aXgEGStpK0ATAMmFDmNplZBXNQr2ARUQ18F3gCeBMYHxFTy9sqKzdJDwDPAdtLqpI0otxtssrhaQLMzDLEPXUzswxxUDczyxAHdTOzDHFQNzPLEAd1M7MMcVC3RkmqkfSqpDckPSip8zqc625JJ6TrdzQ2OZmkIZL2bcY13pPUo9jyenU+auK1/kfSBU1to1kpOahbIZ9ExG4RsTOwEjgzd2c6k2STRcS3I2JaI1WGAE0O6mbrOwd1a4q/ANumveinJN0PvC6pvaSfS3pJ0muSzgBQ4kZJ0yT9P6BX3YkkPS1pz3T9CEkvS/qnpMmStiT55XF++lfC/pJ6SnoovcZLkvZLj91M0iRJr0i6jfzz5axB0iOS/iFpqqSR9fb9Mm3LZEk907JtJE1Mj/mLpB1a5NM0K4EO5W6AtQ2SOpDM6z4xLRoM7BwRM9PA+O+I2EvShsDfJE0Cdge2B74A9AamAXfWO29P4HbggPRc3SNisaRbgY8i4hdpvfuBayPir5I2J/mW7eeBy4G/RsSVko4G1gjSDfhWeo1OwEuSHoqID4GNgJcj4vuSLkvP/V2SFzyfGRHvSPoScDPwlWZ8jGYl56BuhXSS9Gq6/hdgDEla5MWImJmWHwbsUpcvBzYFBgEHAA9ERA0wR9Kf85x/b+DZunNFREPzhB8C7Cit7oh3kbRJeo2vp8f+P0lLirincyUdl64PTNv6IVAL/DYtvxf4vaSN0/t9MOfaGxZxDbOycFC3Qj6JiN1yC9LgtiK3CDgnIp6oV+8oCk8VrCLqQJIq3CciPsnTlqLnupA0hOQXxD4R8bGkp4GODVSP9LpL638GZpXKOXVrCU8AZ0n6HICk7SRtBDwLDEtz7n2Bg/Ic+xxwoKSt0mO7p+XLgU1y6k0iSYWQ1tstXX0W+EZadiTQrUBbNwWWpAF9B5K/FOq0A+r+2jiZJK2zDJgp6cT0GpK0a4FrmJWNg7q1hDtI8uUvpy9Dvo3kr8CHgXeA14FbgGfqHxgRC0ny4L+X9E8+S3/8ATiu7kEpcC6wZ/ogdhqfjcK5AjhA0sskaaBZBdo6Eegg6TXgJ8DzOftWADtJ+gdJzvzKtPwbwIi0fVPxKwWtgnmWRjOzDHFP3cwsQxzUzcwyxEHdzCxDHNTNzDLEQd3MLEMc1M3MMsRB3cwsQ/4/w0X18pBxA1YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Mean = 0.095\n",
      "\n",
      "__________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "ACCURACY METRIC OF ARTIFICIAL NEURAL NETWORK (KERAS)\n",
      "\n",
      "The Confusion Matrix for the ANN Model\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Mean = 0.1125\n"
     ]
    }
   ],
   "source": [
    "# For Manual Method\n",
    "# Output: The Confusion Matrix, Accuracy and the Prediction Mean\n",
    "print(\"\\n\\nACCURACY METRIC OF ARTIFICIAL NEURAL NETWORK (MANUAL)\")\n",
    "print(\"\\nThe Confusion Matrix for the ANN Model\\n\")\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('Accuracy = {0:.2f}%'.format(accuracy))\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "print(\"\\nPrediction Mean = \" + str(np.mean(y_pred)))\n",
    "print(\"\\n__________________________________________________________\\n\")\n",
    "\n",
    "# For Keras Method\n",
    "# Output: The Confusion Matrix, Accuracy and the Prediction Mean\n",
    "print(\"\\n\\nACCURACY METRIC OF ARTIFICIAL NEURAL NETWORK (KERAS)\")\n",
    "print(\"\\nThe Confusion Matrix for the ANN Model\\n\")\n",
    "sns.heatmap(cm1, annot=True, fmt='g')\n",
    "plt.title('Accuracy = {0:.2f}%'.format(accuracy1*100))\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "print(\"\\nPrediction Mean = \" + str(np.mean(y_pred1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On comparison, we can see that ANN Model using own code is less accurate as compared to ANN Model with Keras. But, both models can be considered as a good fit for prediction of unknown values. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
