{"cells":[{"cell_type":"markdown","metadata":{"id":"8TSTlzFpu9WN"},"source":["### Exercise-9\n","### Python Program for Artificial Neural Network (Manual)."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6dE36LXu9WR","executionInfo":{"status":"ok","timestamp":1674210791523,"user_tz":-330,"elapsed":1823,"user":{"displayName":"Souvik Ghosh","userId":"00747165993310192527"}},"outputId":"8bc1a9f3-5a0c-456e-a99b-2514876e1023"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/128 : \t loss:0.693171\n","Epoch 2/128 : \t loss:0.559884\n","Epoch 3/128 : \t loss:0.523445\n","Epoch 4/128 : \t loss:0.511414\n","Epoch 5/128 : \t loss:0.506760\n","Epoch 6/128 : \t loss:0.504578\n","Epoch 7/128 : \t loss:0.503120\n","Epoch 8/128 : \t loss:0.501564\n","Epoch 9/128 : \t loss:0.499329\n","Epoch 10/128 : \t loss:0.495829\n","Epoch 11/128 : \t loss:0.490441\n","Epoch 12/128 : \t loss:0.482719\n","Epoch 13/128 : \t loss:0.472824\n","Epoch 14/128 : \t loss:0.461878\n","Epoch 15/128 : \t loss:0.451617\n","Epoch 16/128 : \t loss:0.443372\n","Epoch 17/128 : \t loss:0.437473\n","Epoch 18/128 : \t loss:0.433560\n","Epoch 19/128 : \t loss:0.431093\n","Epoch 20/128 : \t loss:0.429586\n","Epoch 21/128 : \t loss:0.428669\n","Epoch 22/128 : \t loss:0.428092\n","Epoch 23/128 : \t loss:0.427699\n","Epoch 24/128 : \t loss:0.427397\n","Epoch 25/128 : \t loss:0.427135\n","Epoch 26/128 : \t loss:0.426880\n","Epoch 27/128 : \t loss:0.426615\n","Epoch 28/128 : \t loss:0.426330\n","Epoch 29/128 : \t loss:0.426018\n","Epoch 30/128 : \t loss:0.425676\n","Epoch 31/128 : \t loss:0.425302\n","Epoch 32/128 : \t loss:0.424895\n","Epoch 33/128 : \t loss:0.424456\n","Epoch 34/128 : \t loss:0.423987\n","Epoch 35/128 : \t loss:0.423488\n","Epoch 36/128 : \t loss:0.422964\n","Epoch 37/128 : \t loss:0.422416\n","Epoch 38/128 : \t loss:0.421847\n","Epoch 39/128 : \t loss:0.421262\n","Epoch 40/128 : \t loss:0.420662\n","Epoch 41/128 : \t loss:0.420053\n","Epoch 42/128 : \t loss:0.419436\n","Epoch 43/128 : \t loss:0.418815\n","Epoch 44/128 : \t loss:0.418193\n","Epoch 45/128 : \t loss:0.417573\n","Epoch 46/128 : \t loss:0.416957\n","Epoch 47/128 : \t loss:0.416347\n","Epoch 48/128 : \t loss:0.415747\n","Epoch 49/128 : \t loss:0.415157\n","Epoch 50/128 : \t loss:0.414579\n","Epoch 51/128 : \t loss:0.414015\n","Epoch 52/128 : \t loss:0.413465\n","Epoch 53/128 : \t loss:0.412931\n","Epoch 54/128 : \t loss:0.412413\n","Epoch 55/128 : \t loss:0.411913\n","Epoch 56/128 : \t loss:0.411429\n","Epoch 57/128 : \t loss:0.410963\n","Epoch 58/128 : \t loss:0.410514\n","Epoch 59/128 : \t loss:0.410083\n","Epoch 60/128 : \t loss:0.409668\n","Epoch 61/128 : \t loss:0.409271\n","Epoch 62/128 : \t loss:0.408890\n","Epoch 63/128 : \t loss:0.408526\n","Epoch 64/128 : \t loss:0.408177\n","Epoch 65/128 : \t loss:0.407843\n","Epoch 66/128 : \t loss:0.407525\n","Epoch 67/128 : \t loss:0.407220\n","Epoch 68/128 : \t loss:0.406930\n","Epoch 69/128 : \t loss:0.406652\n","Epoch 70/128 : \t loss:0.406388\n","Epoch 71/128 : \t loss:0.406135\n","Epoch 72/128 : \t loss:0.405894\n","Epoch 73/128 : \t loss:0.405664\n","Epoch 74/128 : \t loss:0.405445\n","Epoch 75/128 : \t loss:0.405236\n","Epoch 76/128 : \t loss:0.405036\n","Epoch 77/128 : \t loss:0.404846\n","Epoch 78/128 : \t loss:0.404664\n","Epoch 79/128 : \t loss:0.404491\n","Epoch 80/128 : \t loss:0.404325\n","Epoch 81/128 : \t loss:0.404167\n","Epoch 82/128 : \t loss:0.404015\n","Epoch 83/128 : \t loss:0.403871\n","Epoch 84/128 : \t loss:0.403732\n","Epoch 85/128 : \t loss:0.403600\n","Epoch 86/128 : \t loss:0.403473\n","Epoch 87/128 : \t loss:0.403352\n","Epoch 88/128 : \t loss:0.403236\n","Epoch 89/128 : \t loss:0.403125\n","Epoch 90/128 : \t loss:0.403018\n","Epoch 91/128 : \t loss:0.402915\n","Epoch 92/128 : \t loss:0.402816\n","Epoch 93/128 : \t loss:0.402721\n","Epoch 94/128 : \t loss:0.402630\n","Epoch 95/128 : \t loss:0.402542\n","Epoch 96/128 : \t loss:0.402458\n","Epoch 97/128 : \t loss:0.402376\n","Epoch 98/128 : \t loss:0.402297\n","Epoch 99/128 : \t loss:0.402221\n","Epoch 100/128 : \t loss:0.402147\n","Epoch 101/128 : \t loss:0.402076\n","Epoch 102/128 : \t loss:0.402006\n","Epoch 103/128 : \t loss:0.401939\n","Epoch 104/128 : \t loss:0.401874\n","Epoch 105/128 : \t loss:0.401811\n","Epoch 106/128 : \t loss:0.401749\n","Epoch 107/128 : \t loss:0.401689\n","Epoch 108/128 : \t loss:0.401631\n","Epoch 109/128 : \t loss:0.401573\n","Epoch 110/128 : \t loss:0.401518\n","Epoch 111/128 : \t loss:0.401463\n","Epoch 112/128 : \t loss:0.401409\n","Epoch 113/128 : \t loss:0.401357\n","Epoch 114/128 : \t loss:0.401306\n","Epoch 115/128 : \t loss:0.401255\n","Epoch 116/128 : \t loss:0.401205\n","Epoch 117/128 : \t loss:0.401156\n","Epoch 118/128 : \t loss:0.401108\n","Epoch 119/128 : \t loss:0.401060\n","Epoch 120/128 : \t loss:0.401014\n","Epoch 121/128 : \t loss:0.400967\n","Epoch 122/128 : \t loss:0.400921\n","Epoch 123/128 : \t loss:0.400876\n","Epoch 124/128 : \t loss:0.400831\n","Epoch 125/128 : \t loss:0.400786\n","Epoch 126/128 : \t loss:0.400742\n","Epoch 127/128 : \t loss:0.400698\n","Epoch 128/128 : \t loss:0.400655\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from numpy.random import seed\n","seed(1)\n","import tensorflow \n","tensorflow.random.set_seed(1)\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n","from sklearn.compose import ColumnTransformer\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import Dense\n","import seaborn as sns\n","from sklearn import metrics\n","\n","# Functions for the ANN Model\n","def initialize_parameters(n_x,n_h,n_y):\n","    W1=np.random.randn(n_h,n_x)*0.01\n","    b1=np.zeros((n_h,1))\n","    W2=np.random.randn(n_y,n_h)*0.01\n","    b2=np.zeros((n_y,1))\n","    parameters={\n","            \"W1\":W1,\n","            \"b1\":b1,\n","            \"W2\":W2,\n","            \"b2\":b2}\n","    return parameters\n","\n","def layer_sizes(X,Y):\n","    n_x=X.shape[0]\n","    n_h=10\n","    n_y=Y.shape[0]\n","    return (n_x,n_h,n_y)\n","\n","def compute_cost(A2,Y,parameters):\n","    m=Y.shape[1]\n","    logprobs=np.multiply(np.log(A2),Y)+np.multiply(np.log(1-A2),(1-Y))\n","    cost=-(1/m)*np.sum(logprobs)\n","    return cost\n","\n","def forward_propagation(X,parameters):\n","    W1=parameters[\"W1\"]\n","    b1=parameters[\"b1\"]\n","    W2=parameters[\"W2\"]\n","    b2=parameters[\"b2\"]\n","    \n","    Z1=np.dot(W1,X)+b1\n","    A1=np.tanh(Z1)\n","    Z2=np.dot(W2,A1)+b2\n","    A2=1/(1+np.exp(-Z2))\n","    \n","    cache={\n","            \"Z1\":Z1,\n","            \"A1\":A1,\n","            \"Z2\":Z2,\n","            \"A2\":A2}\n","    return A2,cache\n","\n","def backward_propagation(parameters,cache,X,Y):\n","    m=X.shape[1]\n","    W1=parameters[\"W1\"]\n","    b1=parameters[\"b1\"]\n","    W2=parameters[\"W2\"]\n","    b2=parameters[\"b2\"]\n","    \n","    A1=cache[\"A1\"]\n","    A2=cache[\"A2\"]\n","    \n","    dZ2=A2-Y\n","    dW2=(1/m)*np.dot(dZ2,A1.T)\n","    db2=(1/m)*np.sum(dZ2,axis=1,keepdims=True)\n","    dZ1 = np.multiply((np.dot(W2.T,dZ2)),(1-np.power(A1,2)))\n","    dW1 = 1/(m)*np.dot(dZ1,X.T)\n","    db1 = 1/(m)*np.sum(dZ1,axis=1,keepdims=True)\n","    \n","    grads = {\"dW1\": dW1,\n","             \"db1\": db1,\n","             \"dW2\": dW2,\n","             \"db2\": db2}\n","    return grads\n","\n","def update_parameters(parameters, grads, learning_rate = 0.1):\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    \n","    dW1 = grads[\"dW1\"]\n","    db1 = grads[\"db1\"]\n","    dW2 = grads[\"dW2\"]\n","    db2 = grads[\"db2\"]\n","    \n","    W1 = W1-((learning_rate)*dW1)\n","    b1 = b1-((learning_rate)*db1)\n","    W2 = W2-((learning_rate)*dW2)\n","    b2 = b2-((learning_rate)*db2)\n","    \n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","    return parameters\n","\n","def predict(parameters, X):\n","    A2, cache = forward_propagation(X,parameters)\n","    predictions = (A2>0.5)*1\n","    return predictions\n","\n","# The ANN Model\n","def nn_model(X, Y, n_h, num_iterations = 128, print_cost=False):\n","    np.random.seed(3)\n","    n_x = layer_sizes(X, Y)[0]\n","    n_y = layer_sizes(X, Y)[2]\n","    \n","    parameters = initialize_parameters(n_x, n_h, n_y)\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    \n","    for i in range(0, num_iterations):\n","         A2, cache = forward_propagation(X,parameters)\n","         cost = compute_cost(A2,Y,parameters)\n","         grads = backward_propagation(parameters,cache,X,Y)\n","         parameters = update_parameters(parameters,grads,2) \n","         if print_cost:\n","            print (\"Epoch %i/%i : \\t loss:%f\" %(i+1,num_iterations, cost))\n","    return parameters\n","\n","# Input: Dataset \n","dataset = pd.read_csv('Churn_Modelling.csv')\n","X = dataset.iloc[:, 3:13].values\n","y = dataset.iloc[:, 13].values\n","\n","# Encoding Categorical Values\n","labelencoder_X = LabelEncoder()\n","X[:, 2] = labelencoder_X.fit_transform(X[:, 2])\n","ct = ColumnTransformer([(\"Geography\", OneHotEncoder(), [1])],\n","                       remainder = 'passthrough')\n","X = ct.fit_transform(X)\n","\n","# Splitting the data into training and testing data \n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n","\n","# Feature Scaling\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","\n","# Preprocessing Data\n","X_train=X_train.T\n","X_test=X_test.T\n","y_train=y_train.reshape(y_train.shape[0],1)\n","y_test=y_test.reshape(y_test.shape[0],1)\n","y_train=y_train.T\n","y_test=y_test.T\n","shape_X=X_train.shape\n","shape_Y=y_train.shape\n","m=X_train.shape[1]\n","\n","# Train the model using the training sets\n","(n_x,n_h,n_y)=layer_sizes(X_train,y_train)\n","parameters=initialize_parameters(n_x,n_h,n_y)\n","A2,cache=forward_propagation(X_train,parameters)\n","grads = backward_propagation(parameters, cache, X_train, y_train)\n","parameters = update_parameters(parameters, grads)\n","parameters = nn_model(X_train, y_train, 10, num_iterations=128,print_cost=True)\n","\n","# Make predictions using the testing set\n","y_pred = predict(parameters, X_test)\n","\n","# Calculating the Confusion Matrix and Accuracy of the Model\n","cm = metrics.confusion_matrix(y_test.T, y_pred.T)\n","accuracy = 100*(cm[0][0]+cm[1][1])/X_test.shape[1]"]},{"cell_type":"markdown","metadata":{"id":"zL130zM9u9WV"},"source":["### Exercise-9\n","### Python Program for Artificial Neural Network (Keras)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GqxiqzECu9WW","outputId":"92e57ed3-e0e5-4fcf-e1f8-25ef9d528c1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/128\n","2000/2000 [==============================] - 4s 1ms/step - loss: 0.4521 - accuracy: 0.7960\n","Epoch 2/128\n","2000/2000 [==============================] - 3s 1ms/step - loss: 0.4223 - accuracy: 0.8223\n","Epoch 3/128\n","2000/2000 [==============================] - 5s 2ms/step - loss: 0.4173 - accuracy: 0.8289\n","Epoch 4/128\n","2000/2000 [==============================] - 4s 2ms/step - loss: 0.4141 - accuracy: 0.8315\n","Epoch 5/128\n","2000/2000 [==============================] - 3s 1ms/step - loss: 0.4109 - accuracy: 0.8346\n","Epoch 6/128\n","2000/2000 [==============================] - 4s 2ms/step - loss: 0.4097 - accuracy: 0.8345\n","Epoch 7/128\n","1480/2000 [=====================>........] - ETA: 0s - loss: 0.4059 - accuracy: 0.8363"]}],"source":["# Input: Dataset \n","dataset = pd.read_csv('Churn_Modelling.csv')\n","X = dataset.iloc[:, 3:13].values\n","y = dataset.iloc[:, 13].values\n","\n","# Encoding Categorical Values\n","labelencoder_X = LabelEncoder()\n","X[:, 2] = labelencoder_X.fit_transform(X[:, 2])\n","ct = ColumnTransformer([(\"Geography\", OneHotEncoder(), [1])],\n","                       remainder = 'passthrough')\n","X = ct.fit_transform(X)\n","\n","# Splitting the data into training and testing data \n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n","\n","# Feature Scaling\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","\n","# The ANN Model\n","def deep_model():\n","    classifier = Sequential()\n","    classifier.add(Dense(units=12, kernel_initializer='uniform',\n","                         activation='relu', input_dim=12))\n","    classifier.add(Dense(units=12, kernel_initializer='uniform',\n","                         activation='relu'))\n","    classifier.add(Dense(units=1, kernel_initializer='uniform', \n","                         activation='sigmoid'))\n","    classifier.compile(optimizer='rmsprop', loss='binary_crossentropy',\n","                       metrics=['accuracy'])\n","    return classifier\n","\n","# Create ANN object\n","classifier = deep_model()\n","\n","# Train the model using the training sets\n","classifier.fit(X_train, y_train, batch_size=4, epochs=128)\n","\n","# Make predictions using the testing set\n","y_pred1 = classifier.predict(X_test)\n","y_pred1 = (y_pred1 > 0.5)\n","\n","# Calculating the Confusion Matrix and Accuracy of the Model\n","cm1 = metrics.confusion_matrix(y_test, y_pred1)\n","accuracy1 = (cm1[0][0]+cm1[1][1])/(cm1[0][0]+cm1[0][1]+cm1[1][0]+cm1[1][1])"]},{"cell_type":"markdown","metadata":{"id":"cTheKUzwu9WX"},"source":["### Exercise-9 \n","### Output and Comparison of Both Methods."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdSF4ttpu9WX"},"outputs":[],"source":["# For Manual Method\n","# Output: The Confusion Matrix, Accuracy and the Prediction Mean\n","print(\"\\n\\nACCURACY METRIC OF ARTIFICIAL NEURAL NETWORK (MANUAL)\")\n","print(\"\\nThe Confusion Matrix for the ANN Model\\n\")\n","sns.heatmap(cm, annot=True, fmt='g')\n","plt.title('Accuracy = {0:.2f}%'.format(accuracy))\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')\n","plt.show()\n","print(\"\\nPrediction Mean = \" + str(np.mean(y_pred)))\n","print(\"\\n__________________________________________________________\\n\")\n","\n","# For Keras Method\n","# Output: The Confusion Matrix, Accuracy and the Prediction Mean\n","print(\"\\n\\nACCURACY METRIC OF ARTIFICIAL NEURAL NETWORK (KERAS)\")\n","print(\"\\nThe Confusion Matrix for the ANN Model\\n\")\n","sns.heatmap(cm1, annot=True, fmt='g')\n","plt.title('Accuracy = {0:.2f}%'.format(accuracy1*100))\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')\n","plt.show()\n","print(\"\\nPrediction Mean = \" + str(np.mean(y_pred1)))"]},{"cell_type":"markdown","metadata":{"id":"D-_lgpVsu9WX"},"source":["##### On comparison, we can see that ANN Model using own code is less accurate as compared to ANN Model with Keras. But, both models can be considered as a good fit for prediction of unknown values. "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}